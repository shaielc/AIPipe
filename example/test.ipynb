{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.stt.whisper import FasterWhisperBackend\n",
    "from modules.stt.stage import STTStage\n",
    "\n",
    "stt_backend = FasterWhisperBackend(\"large-v3\", \"cuda\", \"int8\")\n",
    "stt_stage = STTStage(backend=stt_backend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.LLM.llama_cpp import LlamaCPPServerBackend\n",
    "from modules.LLM.stage import LLMStage\n",
    "\n",
    "llm = LlamaCPPServerBackend(\"http://localhost:8080\")\n",
    "llm_stage = LLMStage(backend=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt_stage.connect(llm_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e9e75a5f33420d94a147a2d78cb77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ChatWidget(children=(ChatDisplay(layout=Layout(border_bottom='1px solid black', border_left='1px solid black',â€¦"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jupyter_utility_widgets.chat.chat_interface import ChatWidget\n",
    "from jupyter_utility_widgets.chat.message import ContentType, TextMessage, Message, RawAudioMessage\n",
    "from tpipes.data import Data\n",
    "from tpipes.pipeline import run_pipeline\n",
    "from datatypes.content_types import RawAudioContent, StringContent\n",
    "\n",
    "def add_llm_response(data):\n",
    "    chat.add_message(TextMessage(content=\"LLM: \" + data.content.value, source=chat.ASSISTANT_KEY))\n",
    "\n",
    "def raw_audio_input_pipeline(message: RawAudioMessage):\n",
    "    data = Data(content=RawAudioContent(value=message.content.data, sample_rate=message.content.sample_rate), metadata={})\n",
    "    pipeline_iter = run_pipeline(stt_stage, input_data=data)\n",
    "    data = next(pipeline_iter)\n",
    "    # print(data)\n",
    "    if data.error:\n",
    "        raise data.content\n",
    "    chat.add_message(TextMessage(content=\"STT: \"+ data.content.value, source=chat.ASSISTANT_KEY))\n",
    "    data = next(pipeline_iter)\n",
    "    # print(data)\n",
    "    if data.error:\n",
    "        raise data.content\n",
    "    add_llm_response(data)\n",
    "\n",
    "def text_input_pipeline(message: TextMessage):\n",
    "    data = Data(content=StringContent(value=message.content), metadata={})\n",
    "    pipeline_iter = run_pipeline(llm_stage, input_data=data)\n",
    "    data = next(pipeline_iter)\n",
    "    # print(data)\n",
    "    if data.error:\n",
    "        raise data.content\n",
    "    add_llm_response(data)\n",
    "\n",
    "def test_pipeline(message: Message):\n",
    "    if message.type == ContentType.RAW_AUDIO:\n",
    "        raw_audio_input_pipeline(message)\n",
    "    elif message.type == ContentType.TEXT:\n",
    "        text_input_pipeline(message)\n",
    "\n",
    "llm.reset()\n",
    "chat = ChatWidget()\n",
    "chat.listen(test_pipeline)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello! How can I assist you today? \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm.process(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': 'remember yes=no',\n",
       "  'assistant': \" I do not store any personal data or retain information about individual users' preferences. Each time you interact with me, it's a new conversation, and I don't retain any context from previous conversations. I'm designed to provide helpful and informative responses based on my training data.\\n\"},\n",
       " {'user': '',\n",
       "  'assistant': ' I can provide information on a wide range of topics, including but not limited to: general knowledge, science, history, technology, entertainment, and more. My training data includes a vast amount of text from various sources, which I use to generate responses.\\n'},\n",
       " {'user': 'I want you to answer the opposite of what you mean.',\n",
       "  'assistant': \" I can try to do that, but it's worth noting that my primary goal is to provide accurate and helpful information. While I can attempt to provide the opposite of a statement, my responses are ultimately based on my understanding of the topic and the information available to me.\\n\"},\n",
       " {'user': 'Are you human?',\n",
       "  'assistant': \" No, I am not human. I am a computer program designed to simulate conversation and answer questions to the best of my ability based on my training data. I'm a type of artificial intelligence (AI) designed to provide helpful and informative responses.\\n\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix submit.\n",
    "# TODO: Add TTS\n",
    "## TODO: make everything available on GitHub\n",
    "## TODO: move to Jetson\n",
    "## TODO: ensure you can connect vscode to the jetson\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
